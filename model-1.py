# -*- coding: utf-8 -*-
"""generate_embeddings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16V1ogATBWL6pYrv8NFncAphkfXgE3faj
"""

import torch

# If there's a GPU available...
if torch.cuda.is_available():    

    # Tell PyTorch to use the GPU.    
    device = torch.device("cuda:3")

    print('There are %d GPU(s) available.' % torch.cuda.device_count())

    print('We will use the GPU:', torch.cuda.get_device_name(0))

# If not...
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")


import json
import numpy as np
import os
import random
import re
import pickle
import torch
from tqdm.autonotebook import tqdm

# Load pre-trained model tokenizer (vocabulary)
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('stsb-roberta-large')

os.environ['TOKENIZERS_PARALLELISM']='False'

from torch.utils.data import Dataset, DataLoader

class Data(Dataset):
    def __init__(self,embed,aspects):

        self.labels=aspects
        self.embed=embed

        self.size=embed.size()[0]


    @classmethod
    def getReader(cls,low,up):

        with open("/DATA/kartik_1901ce16/Review_Exhaustiveness/aspp.pkl",'rb') as out:
            aspp=pickle.load(out)

        with open("/DATA/kartik_1901ce16/Review_Exhaustiveness/r_s.json",'r') as out:
            data_s1=json.load(out)

        '''with open('/DATA/kartik_1901ce16/Review_Exhaustiveness/keys.pkl', 'rb') as out:
            keys=pickle.load(out)'''

        keys = [i for i in range(25000)]
        keys1 = keys[low:up]

        #keys1 = [int(k) for k in keys]'''
        aspp=torch.Tensor(aspp)
        labels=aspp[keys1]
        data_s = [data_s1[str(a)] for a in keys1]

        assert len(labels) == len(data_s)
        assert torch.equal(labels[1], aspp[keys1[1]])

        print("Total number of Reviews", len(labels))

        pbar = tqdm(data_s)

        embeds=[]
        index = 0
        for v in pbar:
          pbar.set_description("Reading Embeddings")
          if len(v)<=13:
            v=v+[""]*(13-len(v))
          else:
            v=v[0:13]
          encoded=model.encode(v, show_progress_bar=False)
          embeds.append(encoded)
          index+=1
      
        embeds = torch.tensor(embeds)

        return cls(embeds, labels)


    def __getitem__(self,idx):

        data=self.embed[idx]
        l=self.labels[idx]

        return data,l


    def __len__(self):
        return self.size

def getLoaders (batch_size):

        print('Reading the training Dataset...')
        print()
        train_dataset = Data.getReader(0,16000) #5984(for batch_size=32) 5952(for batch_size=64) 5888(for batch_size=128) 18752(for 25k)
        
        print()

        print('Reading the validation Dataset...')
        print()
        valid_dataset = Data.getReader(16000,22016) #8000 #24812
        
        trainloader = DataLoader(dataset=train_dataset, batch_size = batch_size,shuffle=False, num_workers=8)
        validloader = DataLoader(dataset=valid_dataset, batch_size = batch_size,shuffle=False, num_workers=8)
        
        return trainloader, validloader

trainloader,validloader = getLoaders(batch_size=32)

print("Length of TrainLoader:",len(trainloader))
print("Length of ValidLoader:",len(validloader))

import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class BertModel(nn.Module):
    def __init__(self, in_features, out_features):

        super(BertModel, self).__init__()

        self.in_features = in_features   #768
        self.out_features = out_features    #7

        self.flatten=nn.Flatten()

        self.lstm_1 = nn.LSTM(in_features, 1792//2, 2, batch_first=True, bidirectional=True) #bidirectional=True
        self.lstm_2 = nn.LSTM(1792, 1536//2, 1, batch_first=True, bidirectional=True) #bidirectional=True
        self.linear0=nn.Linear(1536, 1280)
        self.linear1=nn.Linear(1280, 1024)
        self.linear2=nn.Linear(1024, 768)
        self.linear3=nn.Linear(768, 512)
        self.linear4=nn.Linear(512, 256)
        self.linear5=nn.Linear(256,128)
        self.linear6=nn.Linear(128,64)
        self.last_dense = nn.Linear(64, 32)
        self.last_dense1 = nn.Linear(32, self.out_features)
        self.dropout=nn.Dropout(p=0.4)

        self.gelu = nn.GELU()
        #self.relu = nn.ReLU()
        self.sigmoid=nn.Sigmoid()

        category = torch.rand(1536, out_features,requires_grad=True)  #(512,7)
        nn.init.xavier_normal_(category)
        #self.category=category
        self.category=category.to(device)

    def forward(self, review):

        #input_size - (32,13,768)   (n_r,n_m_s,dim)
        #n_r = review.shape[0]
        s_e=review                  #(32,13,768)

        h0 = torch.zeros(4, s_e.size(0), 1792 // 2)
        c0 = torch.zeros(4, s_e.size(0), 1792 // 2)
        h0, c0 = h0.to(device), c0.to(device)
        s_e, (hn, cn) = self.lstm_1(s_e, (h0, c0))    #(32,13,512)

        h0_ = torch.zeros(2, s_e.size(0), 1536 // 2)
        c0_ = torch.zeros(2, s_e.size(0), 1536 // 2)
        h0_, c0_ = h0_.to(device), c0_.to(device)
        s_e, (hn_, cn_) = self.lstm_2(s_e, (h0_, c0_))    #(32,13,512)

        c=self.category             #(512,7)
        
        comp = torch.cat([torch.unsqueeze(torch.matmul(s,c), dim=0) for s in s_e], dim=0)   #(32,13,7)

        wts = F.softmax(comp, dim=2) #(32,13,7)

        wts=wts.permute(0,2,1)      #(32,7,13)
        e=torch.bmm(wts,s_e)        #(32,7,512)
        r_embed=torch.mean(e,dim=1) #(32,512)
        l = r_embed
        #l = l.view(l.size(0), -1)

        l = self.gelu(self.linear0(l))
        l = self.gelu(self.linear1(l))
        l = self.gelu(self.linear2(l))
        l = self.gelu(self.linear3(l))
        l = self.gelu(self.linear4(l))
        #l = self.dropout(l)
        l = self.gelu(self.linear5(l))
        l = self.gelu(self.linear6(l))
        l = self.gelu(self.last_dense(l))
        model_output = self.sigmoid(self.last_dense1(l))
      
        return model_output, wts

text_model = BertModel(1024,7)
text_model.to(device)
criterion = nn.BCELoss()
optimizer = torch.optim.SGD(text_model.parameters(), lr=0.001, momentum=0.9)

text_model.train()
result=[]
EPOCH=35

train_out = []
val_out = []
train_true = []
val_true = []
attn_train = []
attn_val = []

train_losses = []
val_losses = []

for epoch in range(35):
    #acc_log=[]
    #f1_log=[]
    final_train_loss=0.0
    final_val_loss=0.0
    text_model.train()

    for d in tqdm((trainloader),desc="Train epoch {}/{}".format(epoch + 1, EPOCH)):
        
        x,y_train = d
        x, y_train = x.to(device), y_train.to(device)
        optimizer.zero_grad()
        out, attn_t = text_model(x)
        if (epoch+1 == EPOCH):
          train_out.append(torch.transpose(out,0,1))
          train_true.append(torch.transpose(y_train,0,1))
          #attn_train.append(torch.tensor(attn_t))
          #attn_train = torch.tensor(attn_train)
        loss = criterion(out, y_train)
        loss.backward()
        optimizer.step()
        final_train_loss +=loss.item()
    
    train_losses.append(final_train_loss / len(trainloader))

    #val_acc_log=[]
    #val_loss_log=[]
    #val_f1_log=[]
    
    text_model.eval()

    for i,d in enumerate(validloader):
        x_val,y_val=d
        x_val, y_val = x_val.to(device), y_val.to(device)
        
        out_val, attn_v = text_model(x_val)
        if (epoch+1 == EPOCH):
          val_out.append(torch.transpose(out_val,0,1))
          val_true.append(torch.transpose(y_val,0,1))
          attn_val.append(attn_v.clone().detach())
          #attn_val = torch.tensor(attn_val)
        loss = criterion(out_val, y_val)
        final_val_loss+=loss.item()

    val_losses.append(final_val_loss / len(validloader))
        #f_0,a_0=multi_acc(out_val,y_val)
        #val_acc_log.append(a_0)
        #val_f1_log.append(f_0)
        #op=(out_val,y_val)
        #result.append(op)
    
    curr_lr = optimizer.param_groups[0]['lr']

    #print(f"Epoch {epoch+1}, loss: { final_train_loss/len(trainloader)}, train_accuracy: {np.average(acc_log)}, train_f1:  {np.average(f1_log)}")
    print("Epoch {}, loss: {}, val_loss: {}".format(epoch+1, final_train_loss/len(trainloader), final_val_loss/len(validloader)))
    #print(f"val_losss: { final_val_loss/len(validloader)}, val_accuracy: {np.average(val_acc_log)},val_f1:  {np.average(val_f1_log)}")
    #print(f"val_loss: { final_val_loss/len(validloader)}")
    print()

print(train_losses)
print(val_losses)

'''torch.save(optimizer.state_dict(), "/DATA/kartik_1901ce16/Review_Exhaustiveness/optim_state_dict1.pt")

torch.save(text_model.state_dict(), "/DATA/kartik_1901ce16/Review_Exhaustiveness/model_state_dict1.pt")

torch.save(text_model, "/DATA/kartik_1901ce16/Review_Exhaustiveness/model1.pt")'''

train_out = torch.cat(train_out, 1)
val_out = torch.cat(val_out, 1)
train_true = torch.cat(train_true, 1)
val_true = torch.cat(val_true, 1)
'''attn_train = torch.cat(attn_train, 0)'''
attn_val = torch.cat(attn_val, 0)

train_out, val_out, train_true, val_true = train_out.cpu(), val_out.cpu(), train_true.cpu(), val_true.cpu()
attn_val = attn_val.cpu()

'''val_out_ = (val_out, val_true)
val_outs = open('/DATA/kartik_1901ce16/Review_Exhaustiveness/val_large_bilstm.pkl', 'wb')
pickle.dump(val_out_, val_outs)'''

'''attn_ = (attn_val)
attn_mat = open('/DATA/kartik_1901ce16/Review_Exhaustiveness/attention_sent.pkl', 'wb')
pickle.dump(attn_, attn_mat)
attn_mat.close()'''

#print(train_out.shape)

def labelwise_metrics(pred, true):

  pred = (pred>0.425)

  batch_size = len(pred)

  pred = torch.tensor(pred, dtype=int)
  true = torch.tensor(true, dtype=int)
  #pred = torch.transpose(pred, 0, 1)
  #true = torch.transpose(true, 0, 1)

  from sklearn.metrics import accuracy_score

  for i in range(batch_size):
    acc=accuracy_score(true[i],pred[i])

    epsilon = 1e-7
    confusion_vector = pred[i]/true[i]

    true_positives = torch.sum(confusion_vector == 1).item()
    false_positives = torch.sum(confusion_vector == float('inf')).item()
    true_negatives = torch.sum(torch.isnan(confusion_vector)).item()
    false_negatives = torch.sum(confusion_vector == 0).item()

    precision = true_positives/(true_positives+false_positives+epsilon)
    recall = true_positives/(true_positives+false_negatives+epsilon)
    f1 = 2*precision*recall/(precision+recall+epsilon)

    print("Label: {}, acc: {:.3f}, f1: {:.3f}".format(i+1, acc, f1))

  return 0


print('Training...')
labelwise_metrics(train_out, train_true)
print()
print('Validation...')
labelwise_metrics(val_out, val_true)

"""
def extract(file):

  with open(file,'r') as out:
    reviews=json.load(out) 

  embeds = []
  for k, v in reviews.items():
    v = v.split('.')
    v=v+[""]*(27-len(v))
    encoded=model.encode(v, show_progress_bar=False)
    embeds.append(encoded)
  
  embeds = torch.tensor(embeds)

  text_model.eval()
  extracted = text_model(embeds)
  print(extracted)


extract('/DATA/kartik_1901ce16/Review_Exhaustiveness/reviews.json')

a = train_out[0]
#a = torch.tensor(a, dtype=float)
print(a)

print(train_true[0])

import torch

a = torch.randn(32, 7)
b = torch.randn(32, 7)
print(a)
print(b)
d = torch.cat((torch.transpose(a,0,1),torch.transpose(b,0,1)), 1)
print(d.shape)

t = [[1,0,1],
     [1,1,1],
     [0,0,0],
     [0,1,0]]

p = [[1,1,1],
     [0,0,1],
     [1,1,1],
     [0,1,1]]

t = torch.tensor(t)
p = torch.tensor(p)

labelwise_metrics(p, t)

names = ['motivation', 'clarity', 'soundness', 'substance', 'meaningful', 'originality', 'replicability']

"""
